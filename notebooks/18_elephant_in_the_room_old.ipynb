{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18_elephant in the room.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_382Cp9e_5FN"
      },
      "source": [
        "# Using a pretrained Imagenet network to predict images into one of the 1000 Imagenet classes\n",
        "\n",
        "In this notebook you will learn how load a pretrained Imagenet network. You will see how you can download iamges from the web and resize it to the corresponding input size of the network. In addition, you will use the orginal preprocessing of the VGG16 network. You will use to the network to classify some images into one of 1000 classes. You will fist use rather clear and obvious examples for a dog (affenpinscher) and an elephant (tusker) and then you will use a quite unusual image of an elephant inside a building from the Smithsonian Museum of Natural History in Washington DC, which the pretrained network probably never saw in that way because it was not part of the Imagenet training dataset. Will the VGG16 network be able to predict the unusual image into the correct class?\n",
        "\n",
        "\n",
        "\n",
        "**Content:**\n",
        "* Load the pretrained VGG16 network that was trained on the 1000 classes of Imagenet\n",
        "* Download and resize iamges from urls\n",
        "* Define a function to apply the original preprocessing that the VGG team used when they trained the network\n",
        "* Define a function to undo the original preprocessing (to be able to plot the image afterwards, if necessary)\n",
        "* Predict the two clear examples of a dog and an elephant and decode the predictions into the corresponding lables\n",
        "* Predict the unusual image of the elephant in the museum and decode the predictions into the corresponding lables\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tqg3DtxjaNKr",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2D6GvXYPG3ID"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hfvmZgeVaRVu",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "\n",
        "print(\"TF  Version\",tf.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xuVkV9xeG6H-"
      },
      "source": [
        "### Loading the pretrained VGG16 network, trained on the large Imagetnet dataset \n",
        "In the next cells you download the pretrained VGG16 network, you specify inclide_top = True, because you will use the network for classification and not for feature extraction. The weights = \"imagenet\" means that you want to use the pretrained weights and not random weights. When you print the model summary, you can see that the network input size is 224x224x3, so you will need to resize the images into that size. The output is 1000 which corresponds to the probabilities for the 1000 classes, in between we have convolution, maxpooling and dense layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7_Lbm7znaRbP",
        "colab": {}
      },
      "source": [
        "# The pretrained VGG16 network need quite some memory, \n",
        "# make sure you have enough memory allocated for docker if you are running this notebook locally\n",
        "\n",
        "model_vgg=tf.keras.applications.vgg16.VGG16(include_top=True, weights='imagenet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kv2VA8MIaRhT",
        "colab": {}
      },
      "source": [
        "model_vgg.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pd9GxKxpJokZ"
      },
      "source": [
        "In the next cell you define two function to preprocess the input image and to undo the preprocessing. The preprocessing is very simple, it is just substracting the mean value of every channel, the mean values for the channels are calculated on the Imagenet training dataset. Note that we first need to shift the channels around because the VGG team used the BGR and not the RGB format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2d0vv8fmH530",
        "colab": {}
      },
      "source": [
        "def preprocess_input(img):\n",
        "  x=np.zeros((224,224,3),dtype=\"float32\")\n",
        "  x[:,:,0]=img[:,:,2]\n",
        "  x[:,:,1]=img[:,:,1]\n",
        "  x[:,:,2]=img[:,:,0]\n",
        "  mean = [103.939, 116.779, 123.68]\n",
        "  x[:,:, 0] = x[:,:, 0]-mean[0]\n",
        "  x[:,:, 1] = x[:,:, 1]-mean[1]\n",
        "  x[:,:, 2] = x[:,:, 2]-mean[2]\n",
        "  return x \n",
        "\n",
        "def undo_preprocess_input(img):\n",
        "  mean = [103.939, 116.779, 123.68]\n",
        "  img[:,:, 0] = img[:,:, 0]+mean[0]\n",
        "  img[:,:, 1] = img[:,:, 1]+mean[1]\n",
        "  img[:,:, 2] = img[:,:, 2]+mean[2]\n",
        "  x=np.zeros((224,224,3),dtype=\"float32\")\n",
        "  x[:,:,0]=img[:,:,2]\n",
        "  x[:,:,1]=img[:,:,1]\n",
        "  x[:,:,2]=img[:,:,0]\n",
        "  return x "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AKp3nKxOKuwf"
      },
      "source": [
        "## Loading two clear images of a dog (affenpinscher) and an elephant (tusker)\n",
        "\n",
        "In the next few cells you will download two images from urls and resize them to the input size of the pretrained VGG16 model which is 224x244x3. You plot them before and after the resizing. As you can see the images are very clear and there should be no problem to classify them into the correct label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWKP0NlYaRY9",
        "colab": {}
      },
      "source": [
        "img1 = (Image.open(urlopen(\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/Affenpinscher_Molly.jpg\")))\n",
        "plt.imshow(img1)\n",
        "plt.show()\n",
        "new_width  = 224\n",
        "new_height = 224\n",
        "img1 = img1.resize((new_width, new_height), Image.ANTIALIAS)\n",
        "plt.imshow(img1)\n",
        "plt.show()\n",
        "img1=np.array(img1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tF6TfO5nJBBM",
        "colab": {}
      },
      "source": [
        "img2 = (Image.open(urlopen(\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/African_Elephant.jpg\")))\n",
        "plt.imshow(img2)\n",
        "plt.show()\n",
        "new_width  = 224\n",
        "new_height = 224\n",
        "img2 = img2.resize((new_width, new_height), Image.ANTIALIAS)\n",
        "plt.imshow(img2)\n",
        "plt.show()\n",
        "img2=np.array(img2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "36jjUtpGJTzx",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img1)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jKoIAb6TMOeP"
      },
      "source": [
        "Now that the images are in the right size, let's use the network to predict the label. Don't forget to preprocess the input image before the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uEADbjYJdfD7",
        "colab": {}
      },
      "source": [
        "img1=preprocess_input(img1)\n",
        "print(img1.shape)\n",
        "img2=preprocess_input(img2)\n",
        "print(img2.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rxLSAbNBdyQ9",
        "colab": {}
      },
      "source": [
        "pred1=model_vgg.predict(np.expand_dims(img1,axis=0))\n",
        "tf.keras.applications.vgg16.decode_predictions(pred1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gqyD2IbfK6py",
        "colab": {}
      },
      "source": [
        "pred2=model_vgg.predict(np.expand_dims(img2,axis=0))\n",
        "tf.keras.applications.vgg16.decode_predictions(pred2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "naNMeXW4NMDu"
      },
      "source": [
        "As you can see the network has no problem to predict the correct label, affenpinscher and tusker are there with a high probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qZtxyozwLGdj"
      },
      "source": [
        "## Loading and predicting \"the elephant in the room\" \n",
        "\n",
        "Let's see if the VGG16 network is also able to predict an image that was not part of the training dataset, in this case an elephant inside a museum. Note that there are a lot of other objects in the image and the lighting is also not very good. Let's load, resize, preprocess and predict the image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ravlwI1MLb9b",
        "colab": {}
      },
      "source": [
        "### Image by mana5280 on Unsplash, Smithsonian Museum of Natural History, Washington DC\n",
        "\n",
        "img = (Image.open(urlopen(\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/mana5280-o69yU0jE0Nk-unsplash.jpg\")))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "new_width  = 224\n",
        "new_height = 224\n",
        "img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img=np.array(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sU41pvX5LcGc",
        "colab": {}
      },
      "source": [
        "img=preprocess_input(img)\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NJG8zG1ELcJP",
        "colab": {}
      },
      "source": [
        "pred=model_vgg.predict(np.expand_dims(img,axis=0))\n",
        "tf.keras.applications.vgg16.decode_predictions(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xx2e42CkQtC8"
      },
      "source": [
        "**You can see that the VGG16 network is not able to predict the elephant in the room (the top prediction is horse cart), even though as a human you have no problem at all to see the elephant! The problem is that this is a quite unusual image and in the Imagenet training dataset there were no elephants inside, they used \"normal\" images of elephants in free wilderness.**\n",
        "\n",
        "**This is a principle weakness of deep learning and machine learning in general. We, as humans, obviously learn differently. No child in the world would not see the elephant in this image once she learned what an elephant looks like.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT9m7iku-fxv",
        "colab_type": "text"
      },
      "source": [
        "#### Optional Exercise:\n",
        "Read in your own image of an animal in a normal or unusual environment and check the predictions.  \n",
        "Can you find an other \"elephant in the room\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AP0YKI7WR9bH",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
